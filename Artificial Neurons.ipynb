{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*As with all my tutorials, I recommend you follow along with the coding portions on your own. That way, you will learn them better.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this tutorial, you will need the MNIST handwritten digits dataset in the same folder as the notebook. Download all four compressed files from here: http://yann.lecun.com/exdb/mnist/. And then uncompress them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from mnist import MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artificial Neural Networks began with the first artificial neuron: the perceptron. Developed in 1958, the perceptron was based on a simplified model of the neurons we all have in our brains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neurons are made of **dendrites** which sense impulses from surrounding neurons and then transmit them through the **axon** to other neurons. If the input impulses pass a specific **threshold**, then the neuron outputs a signal through the **axon**. If they don't pass the threshold, the neuron outputs nothing. In neurons, this signal is all or nothing: effectively a boolean output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perceptron works similarly to a neuron. Instead of bundling together dendrites, a perceptron has an **input vector ($x$)**: each entry of the vector corresponds to an input impulse. Instead of the body of a neuron, the perceptron assigns specific importance to each entry of the vector, called a **weight ($w$)**. And just like a neuron, the perceptron uses a **threshhold ($b$)**, outputting $1$ if the weighted inputs pass the threshold and $0$ if the weighted inputs do not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In its most basic form, we can think of a perceptron as a 2-way linear classifier. If you're not familiar with what that means, I suggest you check out my **Introduction to ML: Classification** doc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def threshold_fn(x, b=0):\n",
    "    return 1 if x > b else 0\n",
    "\n",
    "class Perceptron(object):\n",
    "    def __init___(self, input_dimension, threshold):\n",
    "        self.b = threshold\n",
    "        self.input_dim = input_dimension\n",
    "        self.weights = np.random.rand(self.input_dim, 1)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        score = np.matmul(self.weights, x)\n",
    "        pred = threshold_fn(score, b=self.b)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artificial neurons are very similar to the perceptron, but they are more generalized. Rather than use the simple threshold function of the perceptron, they use something called an activation function. Just like in the perceptron, an artificial neuron adds up the weighted inputs. But instead of checking if the sum passes a threshold (outputting 1 if activated, and 0 if not), artificial neurons output the result of an function, called an **activation function**, applied to the weighted sum.\n",
    "\n",
    "We can then represent an artificial neuron as a function, where $h$ is the activation function:\n",
    "\n",
    "$$f(x) = h(\\sum_{i = 1}^N w_i \\cdot x_i) = h(w \\cdot x)$$\n",
    "\n",
    "In the case of a perceptron, we use the simple threshold activation for some threshold **$b$**:\n",
    "\n",
    "$$h(x) =\n",
    "\\begin{cases}\n",
    "1 & \\text{if $w \\cdot x \\geq b$} \\\\\n",
    "0 & \\text{otherwise} \\\\\n",
    "\\end{cases} $$\n",
    "\n",
    "Although this is more expressed with a negative threshold as:\n",
    "\n",
    "$$h(x) =\n",
    "\\begin{cases}\n",
    "1 & \\text{if $w \\cdot x + b \\geq 0$} \\\\\n",
    "0 & \\text{otherwise} \\\\\n",
    "\\end{cases} $$\n",
    "\n",
    "Other activation functions exist, and are important for further applications of artificial neurons to build neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial Neuron Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ArtificialNeuron(object):\n",
    "    def __init__(self, input_dimension, activation):\n",
    "        self.activation = activation\n",
    "        self.input_dim = input_dimension\n",
    "        self.weights = np.random.rand(self.input_dim, 1)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        scr = np.matmul(self.weights.T, x)\n",
    "        pred = self.activation(scr)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning With Artificial Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a moment to think about what an artificial neuron is doing, conceptually.\n",
    "\n",
    "Imagine we wanted to build a machine that could classify if an animal was a dog. We would first think of all the **features** that any animal might have: tail, ears, snout, legs, fur, drool, teeth, woofing, meowing, mooing, obedience, speed, curiosity, etc. Then we would think about which of these features are important in identifying the species **label** of the animal — for instance, obedience is highly indicative of dog-ness, but mooing is not. For each feature-species pair, we can assign a weight that corresponds to their relatedness. In the case of dogs, when presented with a set of features we could multiply each feature by the appropriate dog-weight, add up the scores, and if we had a high number that would mean DOG, otherwise it would be NOT DOG.\n",
    "\n",
    "This is basically how an artificial neuron works.\n",
    "\n",
    "What makes artificial neurons so wonderful is that we don't have to tell it which features correspond to dog-ness and not-dogness. The weights associated with each input are variable. All we need is a clever way to have it automatically learn the relevance of each feature and update the weights accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to learn, we need **data**.\n",
    "\n",
    "( If you're not familiar with machine learning, now would be a great time to read my **Introduction to ML: Data** doc. )\n",
    "\n",
    "One of the earliest uses of these artificial neuron systems was for classifying handwritten digits — 0 through 9 — useful for reading postal codes automatically. In this example, the features would be the pixels of the image, and the labels would be the number that the image corresponded to. We will focus on a simplified version of this.\n",
    "\n",
    "Note that this is a **supervised learning** problem, because we're focusing on a dataset where we have pairs of features and labels.\n",
    "\n",
    "Let's import a well known dataset, the **MNIST Handwritten Digits**, and limit ourselves to using just the 0s and 1s for now. The set consists of 60k training features, 60k training labels, 10k test features, and 10k test labels.\n",
    "\n",
    "For an explanation of how to set up these files, read: https://stackoverflow.com/a/40430149"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mndata = MNIST('//Users/jeremiahsafe/Documents/Data/MNIST Handwritten')\n",
    "\n",
    "trainX, trainY = mndata.load_training()\n",
    "testX, testY = mndata.load_testing()\n",
    "\n",
    "train_idx = []\n",
    "for i, ty in enumerate(trainY):\n",
    "    if ty == 0 or ty == 1:\n",
    "        train_idx.append(i)\n",
    "trainX = [trainX[i] for i in train_idx]\n",
    "trainY = [trainY[i] for i in train_idx]\n",
    "\n",
    "test_idx = []\n",
    "for i, ty in enumerate(testY):\n",
    "    if ty == 0 or ty == 1:\n",
    "        test_idx.append(i)\n",
    "testX = [testX[i] for i in test_idx]\n",
    "testY = [testY[i] for i in test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the training set to train our algorithm. We want our computer to extract as much useful information from the training set as possible. We must COMPLETELY hide the test set from our algorithm: the whole point is to measure how well our algorithm can extrapolate patterns to unseen examples. If we expose the algorithm to the test set in any way, we're letting information leak.\n",
    "\n",
    "*People make mistakes in separating train and test all the time. If you are writing a research paper and run your algorithm on the test set to decide whether to publish or keep working, you're still exposing information from the test set to the algorithm through your human judgment about when to publish. To avoid this problem, researchers will often set aside part of the training set, called a **holdout set**, **validation set**, or **development set** to check their algorithm's performance WITHOUT using the test set.*\n",
    "\n",
    "To make sure we're doing things properly, we will further separate the training set into a training set (11k) and a development set (1.6k)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "devX = trainX[11000:]\n",
    "devY = trainY[11000:]\n",
    "trainX = trainX[:11000]\n",
    "trainY = trainY[:11000]\n",
    "\n",
    "trainX = np.array(trainX).reshape(-1, 784, 1)\n",
    "trainY = np.array(trainY).reshape(-1, 1)\n",
    "devX = np.array(devX).reshape(-1, 784, 1)\n",
    "devY = np.array(devY).reshape(-1, 1)\n",
    "testX = np.array(testX).reshape(-1, 784, 1)\n",
    "testY = np.array(testY).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's re-use our artificial neuron from before. We'll set it up to have randomly selected weights and a threshold-based activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "activation_fn = lambda x: threshold_fn(x, b=0)\n",
    "model = ArtificialNeuron(input_dimension=784, activation=activation_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can ask the artificial neuron to make predictions for all the MNIST images in the test set using the random initialization. We will keep track of how many we get correct, and how many we get wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate(model, xs, ys):\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    for i in range(len(xs)):\n",
    "        x, y = xs[i], ys[i]\n",
    "        prediction = model.predict(x)\n",
    "        if prediction == y:\n",
    "            correct += 1\n",
    "        else:\n",
    "            incorrect += 1\n",
    "    print(\"{} correct, {} incorrect.\".format(correct, incorrect))\n",
    "    \n",
    "evaluate(model, devX, devY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the system did pretty poorly. Because the weights were random initially, we would not expect performance to be much different from guessing: a 50/50 split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we ran all the training data through our system, penalizing it when it did poorly and rewarding it when it did well? After each prediction on training data, we have two pieces of information to work with: the predicted label, and the true label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(trainX)):\n",
    "    x,y = trainX[i], trainY[i]\n",
    "    prediction = model.predict(x)\n",
    "    if prediction == y:\n",
    "        continue\n",
    "    elif prediction == 1:\n",
    "        model.weights -= x\n",
    "    else:\n",
    "        model.weights += x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluate(model, devX, devY)\n",
    "evaluate(model, testX, testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty sweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Combining multiple artificial neurons: **Artificial Neural Networks**\n",
    "- How to use data: **Introduction to ML: Data**\n",
    "- Activation Functions: **Experiments with Activation Functions**\n",
    "- Evaluation: **Introduction to ML: Evaluation**\n",
    "- Learning techniques: **Introduction to ML: Learning**\n",
    "- Common supervised datasets: **World of ML: Datasets**"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py3]",
   "language": "python",
   "name": "Python [py3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
